{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzmPossum/awesome-possum/blob/master/MAKE_A_COPY_OF_THIS_Transcribe_%26_Summarize_Interviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suhaknF2jgqx"
      },
      "outputs": [],
      "source": [
        "# Installing some libraries that we will need. You will need to run this once per session of the notebook.\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install openai\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4_L5A0-OhPe"
      },
      "outputs": [],
      "source": [
        "# This part of the notebooks converts files to mp3. You can skip this part if your file is already mp3.\n",
        "import subprocess\n",
        "\n",
        "CurrentFileName = 'example.mp4' # replace this with your current filename\n",
        "FinalFileName = 'example.mp3' # this will be the filename after the conversion to mp3.\n",
        "\n",
        "try:\n",
        "    subprocess.call(['ffmpeg', '-i', f'{CurrentFileName}', f'{FinalFileName}'])\n",
        "\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    print('Error While Converting Audio')\n",
        "\n",
        "ch = input('Press Enter to Close')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaGJ_19_mMyf"
      },
      "outputs": [],
      "source": [
        "# This part of the code sends your file to whisper and outputs a text file with the speech to text transcript.\n",
        "\n",
        "whisper_output = !whisper \"example.mp3\" --model large --language English # Replace \"example.mp3\" with your filename, note this should be the same as the \"FinalFileName above\"\n",
        "\n",
        "with open('example.txt', 'w') as f: # Give this the same name as your file in the line above but with .txt ending.\n",
        "    for line in whisper_output:\n",
        "        f.write(\"%s\\n\" % line)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In this part we will turn our text into chunks so that it will be easier to manage by chat gpt when we summarize.\n",
        "# For shorter text files you might be able to send the entire file in one go.\n",
        "\n",
        "#Define chunk_size\n",
        "\n",
        "chunk_size = 8000\n",
        "file_name = \"example.txt\" # Replace this with your filename\n",
        "\n",
        "\n",
        "def split_file(file_name, chunk_size):\n",
        "    with open(file_name, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # split the text based on the chunk size\n",
        "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "chunked_text = split_file(file_name,chunk_size)\n",
        "\n",
        "print(chunked_text)\n"
      ],
      "metadata": {
        "id": "X5723zLYF4lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This block of code sends the text file to chat GPT and asks chat GPT to summarize the file.\n",
        "import openai\n",
        "import time\n",
        "\n",
        "openai.api_key = \"INPUT API KEY HERE\" # put your openai api key here.\n",
        "\n",
        "total_chunks = len(chunked_text)\n",
        "\n",
        "modified_chunks = []\n",
        "\n",
        "for i, chunk in enumerate(chunked_text, start=1):\n",
        "    modified_chunk = f\"[START CHUNK {i}/{total_chunks}]\\n{chunk}\\n[END CHUNK {i}/{total_chunks}]\"\n",
        "    modified_chunks.append(modified_chunk)\n",
        "\n",
        "# now, `modified_chunks` contains your text chunks with the appropriate start and end markers\n",
        "chunks_to_send = modified_chunks\n",
        "\n",
        "# Inside this function are the prompts for chat GPT. You can modify the first prompt but leave the second part as is.\n",
        "def process_chunk(chunk, is_last):\n",
        "    if is_last:\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Now you have the entire file. Act as an expert in qualitative text analysis. Produce a summary with the main themes from the text and list a couple of examples under each theme of how the user feels.\"},\n",
        "            {\"role\": \"user\", \"content\": chunk}\n",
        "        ]\n",
        "    else:\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Act as a document loader until you load and remember the content of the next text. There might be multiple files, each file is marked by name in the format ###DOCUMENT NAME. I will send you them by chunks. Each chunk start will be noted as [START CHUNK X/TOTAL] and the end of this chunk will be noted as [END CHUNK X/TOTAL] where x is number of current chunk and total is number of all chunks i will send. I will send you multiple messages with chunks, for each message just reply OK [CHUNKx/Total] don't reply anything else, don't explain the text! Let's begin:\"},\n",
        "            {\"role\": \"user\", \"content\": chunk}\n",
        "        ]\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=messages\n",
        "    )\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "# Now, we process each chunk in turn:\n",
        "for i, chunk in enumerate(chunks_to_send, start=1):\n",
        "    is_last = i == len(chunks_to_send)\n",
        "    result = process_chunk(chunk, is_last)\n",
        "    print(f\"Result for chunk {i}: {result}\")\n",
        "\n",
        "    # Pause for 3 seconds between each request\n",
        "    time.sleep(3)\n",
        "\n"
      ],
      "metadata": {
        "id": "WacMRakhkIag"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNBU/ydMbY9MBxiKdH+vAW7",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}